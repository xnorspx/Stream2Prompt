{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Stream2Prompt A demo project for ISDN3002. Video will be streams to the laptop to be detected using trained YOLOv11 model. The detected objects will be used as prompts to generate text using LLM. Here's the documents for the subsystems.","title":"Home"},{"location":"#welcome-to-stream2prompt","text":"A demo project for ISDN3002. Video will be streams to the laptop to be detected using trained YOLOv11 model. The detected objects will be used as prompts to generate text using LLM. Here's the documents for the subsystems.","title":"Welcome to Stream2Prompt"},{"location":"Model-API/Environment-Setup/","text":"Environment Setup Please refer to the Environment Setup of Model Lab, Model API shares the same environment.","title":"Environment Setup"},{"location":"Model-API/Environment-Setup/#environment-setup","text":"Please refer to the Environment Setup of Model Lab, Model API shares the same environment.","title":"Environment Setup"},{"location":"Model-API/Usage/","text":"Model API Usage Guide This document provides comprehensive usage instructions for the Stream2Prompt Model API, a FastAPI-based object detection service using YOLO11. Overview The Model API provides real-time object detection capabilities through a RESTful interface. It uses an asynchronous prediction system where images are processed in a background thread, allowing for efficient handling of multiple requests. Starting the API Server Run the API server using UV: uv run fastapi run model-api/model-api.py The server will start on http://localhost:8000 by default. API Endpoints 1. Root Endpoint - Health Check GET / Returns detection results from a random warmup image (used for testing/health check). Response Format: { \"detections\": [ { \"class_name\": \"person\", \"confidence\": 0.95 } ] } 2. Upload Image for Prediction POST /predict/ Uploads an image for object detection. The image is processed asynchronously in a background thread. Request: Content-Type : multipart/form-data Parameter : image (file upload) Supported Image Formats: JPEG (.jpg, .jpeg) PNG (.png) BMP (.bmp) TIFF (.tiff) Example using cURL: curl -X POST \"http://localhost:8000/predict/\" \\ -H \"accept: application/json\" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"image=@path/to/your/image.jpg\" Response: { \"status\": \"Image received for prediction\" } 3. Get Prediction Results GET /result/ Retrieves the latest prediction results with detailed detection information. Response Format (Successful Detection): { \"detections\": [ { \"class_name\": \"person\", \"confidence\": 0.95, \"bbox\": [100, 50, 200, 300] }, { \"class_name\": \"car\", \"confidence\": 0.87, \"bbox\": [300, 100, 500, 250] } ], \"timestamp\": 1696780800.123, \"total_objects\": 2 } Response Format (No Objects Detected): { \"detections\": [], \"timestamp\": 1696780800.123, \"total_objects\": 0, \"message\": \"No objects detected in the image\" } Response Format (No Prediction Available): { \"detections\": [], \"timestamp\": null, \"message\": \"No prediction available yet\" } Response Fields Explained Detection Object: class_name : Human-readable name of the detected object class confidence : Detection confidence score (0.0 to 1.0) bbox : Bounding box coordinates as [x1, y1, x2, y2] where: x1, y1 : Top-left corner coordinates x2, y2 : Bottom-right corner coordinates Additional Fields: timestamp : Unix timestamp when the prediction was completed total_objects : Total number of detected objects message : Status or informational message (when applicable) Usage Workflow Basic Usage Pattern: Upload Image : Send POST request to /predict/ with your image Poll Results : Send GET requests to /result/ to retrieve detection results Process Results : Use the returned detection data for your application Example Python Client: import requests import json import time # Upload image for prediction with open('image.jpg', 'rb') as f: response = requests.post( 'http://localhost:8000/predict/', files={'image': f} ) print(f\"Upload status: {response.json()}\") # Wait a moment for processing time.sleep(1) # Get prediction results result = requests.get('http://localhost:8000/result/') data = result.json() print(f\"Detected {data['total_objects']} objects:\") for detection in data['detections']: print(f\"- {detection['class_name']}: {detection['confidence']:.2f} confidence\") print(f\" Location: {detection['bbox']}\") Example JavaScript Client: // Upload image for prediction const formData = new FormData(); formData.append('image', fileInput.files[0]); fetch('http://localhost:8000/predict/', { method: 'POST', body: formData }) .then(response => response.json()) .then(data => console.log('Upload status:', data)); // Get prediction results (after a brief delay) setTimeout(() => { fetch('http://localhost:8000/result/') .then(response => response.json()) .then(data => { console.log(`Detected ${data.total_objects} objects:`); data.detections.forEach(detection => { console.log(`${detection.class_name}: ${detection.confidence.toFixed(2)} confidence`); }); }); }, 1000); Features Asynchronous Processing: Images are processed in a background thread Non-blocking API responses Efficient handling of multiple requests Sorted Results: Detections are automatically sorted by confidence in descending order Highest confidence detections appear first Comprehensive Detection Data: Class names for human readability Confidence scores for reliability assessment Bounding box coordinates for spatial information Timestamps for temporal tracking Interactive API Documentation FastAPI automatically generates interactive API documentation: Swagger UI : Visit http://localhost:8000/docs ReDoc : Visit http://localhost:8000/redoc These interfaces allow you to test the API endpoints directly from your browser. Error Handling The API handles various edge cases: No prediction available : Returns empty detections with appropriate message No objects detected : Returns empty detections with timestamp Invalid image format : FastAPI will return appropriate HTTP error codes Missing image parameter : Returns validation error Performance Considerations Model Loading : The YOLO model is loaded once at startup with warmup Background Processing : Predictions run in a separate thread to avoid blocking Memory Management : Only the latest prediction is kept in memory Response Time : Initial model loading takes time; subsequent predictions are faster Troubleshooting Common Issues: Model not found : Ensure model-lab/yolo11/best.engine exists Slow responses : First prediction includes model loading time Memory issues : Large images may require more system memory Port conflicts : Change the default port if 8000 is in use Debug Information: Check the server logs for detailed error messages and processing information. Webcam Client The project includes a lightweight Python client ( webcam_client.py ) for real-time webcam streaming to the Model API. Features Lightweight Design : Minimal resource usage with no GUI overhead Configurable FPS : Adjustable frame rate for API requests Flexible Input : Supports webcam or video file input Progress Monitoring : Real-time statistics and progress updates Error Handling : Robust network error handling and recovery Installation Requirements The webcam client requires additional dependencies: pip install opencv-python requests Basic Usage Default Configuration (10 FPS, camera 0, localhost:8000): python webcam_client.py Custom Frame Rate: python webcam_client.py --fps 15 Different API Endpoint: python webcam_client.py --endpoint http://192.168.1.100:8000 Different Camera: python webcam_client.py --input 1 # Camera index 1 Video File Input: python webcam_client.py --input /path/to/video.mp4 Combined Options: python webcam_client.py --fps 20 --endpoint http://remote-server:8000 --input 1 Command Line Options Option Description Default --fps Frames per second to send to API 10 --endpoint Model API endpoint URL http://localhost:8000 --input Input source: camera index or video file path 0 Configuration You can modify the default settings by editing the configuration section at the top of webcam_client.py : # Default configuration - modify these values as needed DEFAULT_FPS = 10 # Frames per second to send to API DEFAULT_ENDPOINT = \"http://localhost:8000\" # Model API endpoint DEFAULT_INPUT_SOURCE = 0 # Webcam index (0 = default camera) # Network configuration REQUEST_TIMEOUT = 5.0 # Timeout for API requests (seconds) # Video configuration JPEG_QUALITY = 80 # JPEG compression quality (1-100) Usage Workflow Start the Model API server (see above section) Connect your webcam or prepare your video file Run the webcam client with desired settings Monitor progress through console output Stop with Ctrl+C when finished Example Output Stream2Prompt Webcam Client (Lightweight) ================================================== FPS: 10 Endpoint: http://localhost:8000 Input Source: 0 ================================================== \u2713 API connection successful Capturing from source 0 at 10 FPS Sending frames to http://localhost:8000/predict/ Press Ctrl+C to stop Sent 50 frames in 5.1s (avg 9.8 FPS) Sent 100 frames in 10.2s (avg 9.8 FPS) ^C Stopped by user Statistics: Frames captured: 123 Frames sent successfully: 120 Success rate: 97.6% Average send rate: 9.8 FPS Duration: 12.3 seconds Performance Considerations Frame Rate : Higher FPS increases network traffic and API load Network Latency : Remote endpoints may affect achievable frame rates Image Quality : JPEG quality setting affects file size and upload speed Camera Resolution : Higher resolution increases processing time and bandwidth Troubleshooting Common Issues: Camera not found : Check camera index (try different values: 0, 1, 2...) Ensure camera is not used by another application Low frame rate : Check network connection to API endpoint Reduce FPS setting or JPEG quality Ensure API server has sufficient resources Connection errors : Verify API server is running and accessible Check firewall settings if using remote endpoint Ensure correct endpoint URL format Import errors : Install required dependencies: pip install opencv-python requests Integration Examples Continuous Monitoring Script: import subprocess import time while True: try: # Run webcam client subprocess.run([ 'python', 'webcam_client.py', '--fps', '15', '--endpoint', 'http://your-api-server:8000' ]) except KeyboardInterrupt: break except Exception as e: print(f\"Client crashed: {e}\") time.sleep(5) # Wait before restart Batch Processing Multiple Cameras: # Terminal 1 - Camera 0 python webcam_client.py --input 0 --fps 10 # Terminal 2 - Camera 1 python webcam_client.py --input 1 --fps 10 # Terminal 3 - Video file python webcam_client.py --input security_footage.mp4 --fps 30","title":"Usage"},{"location":"Model-API/Usage/#model-api-usage-guide","text":"This document provides comprehensive usage instructions for the Stream2Prompt Model API, a FastAPI-based object detection service using YOLO11.","title":"Model API Usage Guide"},{"location":"Model-API/Usage/#overview","text":"The Model API provides real-time object detection capabilities through a RESTful interface. It uses an asynchronous prediction system where images are processed in a background thread, allowing for efficient handling of multiple requests.","title":"Overview"},{"location":"Model-API/Usage/#starting-the-api-server","text":"Run the API server using UV: uv run fastapi run model-api/model-api.py The server will start on http://localhost:8000 by default.","title":"Starting the API Server"},{"location":"Model-API/Usage/#api-endpoints","text":"","title":"API Endpoints"},{"location":"Model-API/Usage/#1-root-endpoint-health-check","text":"GET / Returns detection results from a random warmup image (used for testing/health check).","title":"1. Root Endpoint - Health Check"},{"location":"Model-API/Usage/#response-format","text":"{ \"detections\": [ { \"class_name\": \"person\", \"confidence\": 0.95 } ] }","title":"Response Format:"},{"location":"Model-API/Usage/#2-upload-image-for-prediction","text":"POST /predict/ Uploads an image for object detection. The image is processed asynchronously in a background thread.","title":"2. Upload Image for Prediction"},{"location":"Model-API/Usage/#request","text":"Content-Type : multipart/form-data Parameter : image (file upload)","title":"Request:"},{"location":"Model-API/Usage/#supported-image-formats","text":"JPEG (.jpg, .jpeg) PNG (.png) BMP (.bmp) TIFF (.tiff)","title":"Supported Image Formats:"},{"location":"Model-API/Usage/#example-using-curl","text":"curl -X POST \"http://localhost:8000/predict/\" \\ -H \"accept: application/json\" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"image=@path/to/your/image.jpg\"","title":"Example using cURL:"},{"location":"Model-API/Usage/#response","text":"{ \"status\": \"Image received for prediction\" }","title":"Response:"},{"location":"Model-API/Usage/#3-get-prediction-results","text":"GET /result/ Retrieves the latest prediction results with detailed detection information.","title":"3. Get Prediction Results"},{"location":"Model-API/Usage/#response-format-successful-detection","text":"{ \"detections\": [ { \"class_name\": \"person\", \"confidence\": 0.95, \"bbox\": [100, 50, 200, 300] }, { \"class_name\": \"car\", \"confidence\": 0.87, \"bbox\": [300, 100, 500, 250] } ], \"timestamp\": 1696780800.123, \"total_objects\": 2 }","title":"Response Format (Successful Detection):"},{"location":"Model-API/Usage/#response-format-no-objects-detected","text":"{ \"detections\": [], \"timestamp\": 1696780800.123, \"total_objects\": 0, \"message\": \"No objects detected in the image\" }","title":"Response Format (No Objects Detected):"},{"location":"Model-API/Usage/#response-format-no-prediction-available","text":"{ \"detections\": [], \"timestamp\": null, \"message\": \"No prediction available yet\" }","title":"Response Format (No Prediction Available):"},{"location":"Model-API/Usage/#response-fields-explained","text":"","title":"Response Fields Explained"},{"location":"Model-API/Usage/#detection-object","text":"class_name : Human-readable name of the detected object class confidence : Detection confidence score (0.0 to 1.0) bbox : Bounding box coordinates as [x1, y1, x2, y2] where: x1, y1 : Top-left corner coordinates x2, y2 : Bottom-right corner coordinates","title":"Detection Object:"},{"location":"Model-API/Usage/#additional-fields","text":"timestamp : Unix timestamp when the prediction was completed total_objects : Total number of detected objects message : Status or informational message (when applicable)","title":"Additional Fields:"},{"location":"Model-API/Usage/#usage-workflow","text":"","title":"Usage Workflow"},{"location":"Model-API/Usage/#basic-usage-pattern","text":"Upload Image : Send POST request to /predict/ with your image Poll Results : Send GET requests to /result/ to retrieve detection results Process Results : Use the returned detection data for your application","title":"Basic Usage Pattern:"},{"location":"Model-API/Usage/#example-python-client","text":"import requests import json import time # Upload image for prediction with open('image.jpg', 'rb') as f: response = requests.post( 'http://localhost:8000/predict/', files={'image': f} ) print(f\"Upload status: {response.json()}\") # Wait a moment for processing time.sleep(1) # Get prediction results result = requests.get('http://localhost:8000/result/') data = result.json() print(f\"Detected {data['total_objects']} objects:\") for detection in data['detections']: print(f\"- {detection['class_name']}: {detection['confidence']:.2f} confidence\") print(f\" Location: {detection['bbox']}\")","title":"Example Python Client:"},{"location":"Model-API/Usage/#example-javascript-client","text":"// Upload image for prediction const formData = new FormData(); formData.append('image', fileInput.files[0]); fetch('http://localhost:8000/predict/', { method: 'POST', body: formData }) .then(response => response.json()) .then(data => console.log('Upload status:', data)); // Get prediction results (after a brief delay) setTimeout(() => { fetch('http://localhost:8000/result/') .then(response => response.json()) .then(data => { console.log(`Detected ${data.total_objects} objects:`); data.detections.forEach(detection => { console.log(`${detection.class_name}: ${detection.confidence.toFixed(2)} confidence`); }); }); }, 1000);","title":"Example JavaScript Client:"},{"location":"Model-API/Usage/#features","text":"","title":"Features"},{"location":"Model-API/Usage/#asynchronous-processing","text":"Images are processed in a background thread Non-blocking API responses Efficient handling of multiple requests","title":"Asynchronous Processing:"},{"location":"Model-API/Usage/#sorted-results","text":"Detections are automatically sorted by confidence in descending order Highest confidence detections appear first","title":"Sorted Results:"},{"location":"Model-API/Usage/#comprehensive-detection-data","text":"Class names for human readability Confidence scores for reliability assessment Bounding box coordinates for spatial information Timestamps for temporal tracking","title":"Comprehensive Detection Data:"},{"location":"Model-API/Usage/#interactive-api-documentation","text":"FastAPI automatically generates interactive API documentation: Swagger UI : Visit http://localhost:8000/docs ReDoc : Visit http://localhost:8000/redoc These interfaces allow you to test the API endpoints directly from your browser.","title":"Interactive API Documentation"},{"location":"Model-API/Usage/#error-handling","text":"The API handles various edge cases: No prediction available : Returns empty detections with appropriate message No objects detected : Returns empty detections with timestamp Invalid image format : FastAPI will return appropriate HTTP error codes Missing image parameter : Returns validation error","title":"Error Handling"},{"location":"Model-API/Usage/#performance-considerations","text":"Model Loading : The YOLO model is loaded once at startup with warmup Background Processing : Predictions run in a separate thread to avoid blocking Memory Management : Only the latest prediction is kept in memory Response Time : Initial model loading takes time; subsequent predictions are faster","title":"Performance Considerations"},{"location":"Model-API/Usage/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Model-API/Usage/#common-issues","text":"Model not found : Ensure model-lab/yolo11/best.engine exists Slow responses : First prediction includes model loading time Memory issues : Large images may require more system memory Port conflicts : Change the default port if 8000 is in use","title":"Common Issues:"},{"location":"Model-API/Usage/#debug-information","text":"Check the server logs for detailed error messages and processing information.","title":"Debug Information:"},{"location":"Model-API/Usage/#webcam-client","text":"The project includes a lightweight Python client ( webcam_client.py ) for real-time webcam streaming to the Model API.","title":"Webcam Client"},{"location":"Model-API/Usage/#features_1","text":"Lightweight Design : Minimal resource usage with no GUI overhead Configurable FPS : Adjustable frame rate for API requests Flexible Input : Supports webcam or video file input Progress Monitoring : Real-time statistics and progress updates Error Handling : Robust network error handling and recovery","title":"Features"},{"location":"Model-API/Usage/#installation-requirements","text":"The webcam client requires additional dependencies: pip install opencv-python requests","title":"Installation Requirements"},{"location":"Model-API/Usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"Model-API/Usage/#default-configuration-10-fps-camera-0-localhost8000","text":"python webcam_client.py","title":"Default Configuration (10 FPS, camera 0, localhost:8000):"},{"location":"Model-API/Usage/#custom-frame-rate","text":"python webcam_client.py --fps 15","title":"Custom Frame Rate:"},{"location":"Model-API/Usage/#different-api-endpoint","text":"python webcam_client.py --endpoint http://192.168.1.100:8000","title":"Different API Endpoint:"},{"location":"Model-API/Usage/#different-camera","text":"python webcam_client.py --input 1 # Camera index 1","title":"Different Camera:"},{"location":"Model-API/Usage/#video-file-input","text":"python webcam_client.py --input /path/to/video.mp4","title":"Video File Input:"},{"location":"Model-API/Usage/#combined-options","text":"python webcam_client.py --fps 20 --endpoint http://remote-server:8000 --input 1","title":"Combined Options:"},{"location":"Model-API/Usage/#command-line-options","text":"Option Description Default --fps Frames per second to send to API 10 --endpoint Model API endpoint URL http://localhost:8000 --input Input source: camera index or video file path 0","title":"Command Line Options"},{"location":"Model-API/Usage/#configuration","text":"You can modify the default settings by editing the configuration section at the top of webcam_client.py : # Default configuration - modify these values as needed DEFAULT_FPS = 10 # Frames per second to send to API DEFAULT_ENDPOINT = \"http://localhost:8000\" # Model API endpoint DEFAULT_INPUT_SOURCE = 0 # Webcam index (0 = default camera) # Network configuration REQUEST_TIMEOUT = 5.0 # Timeout for API requests (seconds) # Video configuration JPEG_QUALITY = 80 # JPEG compression quality (1-100)","title":"Configuration"},{"location":"Model-API/Usage/#usage-workflow_1","text":"Start the Model API server (see above section) Connect your webcam or prepare your video file Run the webcam client with desired settings Monitor progress through console output Stop with Ctrl+C when finished","title":"Usage Workflow"},{"location":"Model-API/Usage/#example-output","text":"Stream2Prompt Webcam Client (Lightweight) ================================================== FPS: 10 Endpoint: http://localhost:8000 Input Source: 0 ================================================== \u2713 API connection successful Capturing from source 0 at 10 FPS Sending frames to http://localhost:8000/predict/ Press Ctrl+C to stop Sent 50 frames in 5.1s (avg 9.8 FPS) Sent 100 frames in 10.2s (avg 9.8 FPS) ^C Stopped by user Statistics: Frames captured: 123 Frames sent successfully: 120 Success rate: 97.6% Average send rate: 9.8 FPS Duration: 12.3 seconds","title":"Example Output"},{"location":"Model-API/Usage/#performance-considerations_1","text":"Frame Rate : Higher FPS increases network traffic and API load Network Latency : Remote endpoints may affect achievable frame rates Image Quality : JPEG quality setting affects file size and upload speed Camera Resolution : Higher resolution increases processing time and bandwidth","title":"Performance Considerations"},{"location":"Model-API/Usage/#troubleshooting_1","text":"","title":"Troubleshooting"},{"location":"Model-API/Usage/#common-issues_1","text":"Camera not found : Check camera index (try different values: 0, 1, 2...) Ensure camera is not used by another application Low frame rate : Check network connection to API endpoint Reduce FPS setting or JPEG quality Ensure API server has sufficient resources Connection errors : Verify API server is running and accessible Check firewall settings if using remote endpoint Ensure correct endpoint URL format Import errors : Install required dependencies: pip install opencv-python requests","title":"Common Issues:"},{"location":"Model-API/Usage/#integration-examples","text":"","title":"Integration Examples"},{"location":"Model-API/Usage/#continuous-monitoring-script","text":"import subprocess import time while True: try: # Run webcam client subprocess.run([ 'python', 'webcam_client.py', '--fps', '15', '--endpoint', 'http://your-api-server:8000' ]) except KeyboardInterrupt: break except Exception as e: print(f\"Client crashed: {e}\") time.sleep(5) # Wait before restart","title":"Continuous Monitoring Script:"},{"location":"Model-API/Usage/#batch-processing-multiple-cameras","text":"# Terminal 1 - Camera 0 python webcam_client.py --input 0 --fps 10 # Terminal 2 - Camera 1 python webcam_client.py --input 1 --fps 10 # Terminal 3 - Video file python webcam_client.py --input security_footage.mp4 --fps 30","title":"Batch Processing Multiple Cameras:"},{"location":"Model-Lab/Environment-Setup/","text":"Environment Setup This guide provides instructions for setting up a development environment optimized for model training and evaluation. We recommend using WSL2 with a Linux distribution for better compatibility and performance. Prerequisites Before starting, ensure you have: Windows 11 Pro with WSL2 enabled NVIDIA GPU with compatible drivers Administrative privileges on your system Recommended Environment Windows Host System OS : Windows 11 Pro 25H2 CUDA : 13.0 Update 1 WSL2 Environment OS : Fedora Linux 42 (WSL2) Python : 3.11.13 CUDA : 12.8.1 Setup Instructions (WSL Environment) 1. CUDA Installation Important : Ensure you have installed the latest NVIDIA GPU driver and CUDA version mentioned in the recommended environment (Windows host) before proceeding with the WSL setup. sudo dnf update -y sudo dnf config-manager addrepo --from-repofile https://developer.download.nvidia.com/compute/cuda/repos/fedora41/x86_64/cuda-fedora41.repo sudo dnf clean all sudo dnf -y install cuda-toolkit-12-8 2. Python Environment Setup Install pyenv and Dependencies # Install pyenv curl https://pyenv.run | bash # Add pyenv to bashrc echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc echo '[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(pyenv init - bash)\"' >> ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' >> ~/.bashrc source ~/.bashrc # Install build dependencies sudo dnf install gcc openssl-devel bzip2-devel libffi-devel sqlite-devel readline-devel zlib-devel xz-devel sudo dnf install tcl tk tcl-devel tk-devel Build and Install Tcl/Tk 8.6.14 # Build and install Tcl 8.6.14 cd /tmp wget https://prdownloads.sourceforge.net/tcl/tcl8.6.14-src.tar.gz tar -xzf tcl8.6.14-src.tar.gz cd tcl8.6.14/unix ./configure --prefix=/opt/tcltk8.6 make -j$(nproc) sudo make install # Build and install Tk 8.6.14 cd /tmp wget https://prdownloads.sourceforge.net/tcl/tk8.6.14-src.tar.gz tar -xzf tk8.6.14-src.tar.gz cd tk8.6.14/unix ./configure --prefix=/opt/tcltk8.6 --with-tcl=/opt/tcltk8.6/lib make -j$(nproc) sudo make install Install Python and UV Package Manager # Install Python 3.11.13 with pyenv export CPPFLAGS=\"-I/opt/tcltk8.6/include\" export LDFLAGS=\"-L/opt/tcltk8.6/lib -ltcl8.6 -ltk8.6\" pyenv uninstall 3.11.13 # Clean up any existing installation PYTHON_CONFIGURE_OPTS=\"--with-tcltk-includes='-I/opt/tcltk8.6/include' --with-tcltk-libs='/opt/tcltk8.6/lib'\" pyenv install -v 3.11.13 # Install UV package manager curl -LsSf https://astral.sh/uv/install.sh | sh # Install project dependencies uv sync # This will automatically install all required packages 3. Installation Validation Verify that your installation is working correctly by running the following validation scripts: # Check if PyTorch is installed and can utilize CUDA for computing uv run tools/check_cuda.py # Compare processing time between CPU and GPU to verify GPU acceleration uv run tools/time_cpu.py uv run tools/time_gpu.py The GPU should show significantly faster processing times compared to CPU if everything is configured correctly.","title":"Environment Setup"},{"location":"Model-Lab/Environment-Setup/#environment-setup","text":"This guide provides instructions for setting up a development environment optimized for model training and evaluation. We recommend using WSL2 with a Linux distribution for better compatibility and performance.","title":"Environment Setup"},{"location":"Model-Lab/Environment-Setup/#prerequisites","text":"Before starting, ensure you have: Windows 11 Pro with WSL2 enabled NVIDIA GPU with compatible drivers Administrative privileges on your system","title":"Prerequisites"},{"location":"Model-Lab/Environment-Setup/#recommended-environment","text":"","title":"Recommended Environment"},{"location":"Model-Lab/Environment-Setup/#windows-host-system","text":"OS : Windows 11 Pro 25H2 CUDA : 13.0 Update 1","title":"Windows Host System"},{"location":"Model-Lab/Environment-Setup/#wsl2-environment","text":"OS : Fedora Linux 42 (WSL2) Python : 3.11.13 CUDA : 12.8.1","title":"WSL2 Environment"},{"location":"Model-Lab/Environment-Setup/#setup-instructions-wsl-environment","text":"","title":"Setup Instructions (WSL Environment)"},{"location":"Model-Lab/Environment-Setup/#1-cuda-installation","text":"Important : Ensure you have installed the latest NVIDIA GPU driver and CUDA version mentioned in the recommended environment (Windows host) before proceeding with the WSL setup. sudo dnf update -y sudo dnf config-manager addrepo --from-repofile https://developer.download.nvidia.com/compute/cuda/repos/fedora41/x86_64/cuda-fedora41.repo sudo dnf clean all sudo dnf -y install cuda-toolkit-12-8","title":"1. CUDA Installation"},{"location":"Model-Lab/Environment-Setup/#2-python-environment-setup","text":"","title":"2. Python Environment Setup"},{"location":"Model-Lab/Environment-Setup/#install-pyenv-and-dependencies","text":"# Install pyenv curl https://pyenv.run | bash # Add pyenv to bashrc echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc echo '[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(pyenv init - bash)\"' >> ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' >> ~/.bashrc source ~/.bashrc # Install build dependencies sudo dnf install gcc openssl-devel bzip2-devel libffi-devel sqlite-devel readline-devel zlib-devel xz-devel sudo dnf install tcl tk tcl-devel tk-devel","title":"Install pyenv and Dependencies"},{"location":"Model-Lab/Environment-Setup/#build-and-install-tcltk-8614","text":"# Build and install Tcl 8.6.14 cd /tmp wget https://prdownloads.sourceforge.net/tcl/tcl8.6.14-src.tar.gz tar -xzf tcl8.6.14-src.tar.gz cd tcl8.6.14/unix ./configure --prefix=/opt/tcltk8.6 make -j$(nproc) sudo make install # Build and install Tk 8.6.14 cd /tmp wget https://prdownloads.sourceforge.net/tcl/tk8.6.14-src.tar.gz tar -xzf tk8.6.14-src.tar.gz cd tk8.6.14/unix ./configure --prefix=/opt/tcltk8.6 --with-tcl=/opt/tcltk8.6/lib make -j$(nproc) sudo make install","title":"Build and Install Tcl/Tk 8.6.14"},{"location":"Model-Lab/Environment-Setup/#install-python-and-uv-package-manager","text":"# Install Python 3.11.13 with pyenv export CPPFLAGS=\"-I/opt/tcltk8.6/include\" export LDFLAGS=\"-L/opt/tcltk8.6/lib -ltcl8.6 -ltk8.6\" pyenv uninstall 3.11.13 # Clean up any existing installation PYTHON_CONFIGURE_OPTS=\"--with-tcltk-includes='-I/opt/tcltk8.6/include' --with-tcltk-libs='/opt/tcltk8.6/lib'\" pyenv install -v 3.11.13 # Install UV package manager curl -LsSf https://astral.sh/uv/install.sh | sh # Install project dependencies uv sync # This will automatically install all required packages","title":"Install Python and UV Package Manager"},{"location":"Model-Lab/Environment-Setup/#3-installation-validation","text":"Verify that your installation is working correctly by running the following validation scripts: # Check if PyTorch is installed and can utilize CUDA for computing uv run tools/check_cuda.py # Compare processing time between CPU and GPU to verify GPU acceleration uv run tools/time_cpu.py uv run tools/time_gpu.py The GPU should show significantly faster processing times compared to CPU if everything is configured correctly.","title":"3. Installation Validation"},{"location":"Model-Lab/Usage/","text":"Usage This guide provides instructions for using the components of the Model Lab, including data labeling, model training, and evaluation. Data Labeling To prepare a new dataset for training, start Label Studio with the following command: uv run label-studio start This will launch the Label Studio web interface for data annotation and labeling. Model Training Configure the training parameters in model-lab/model-training.py Start the training process: uv run model-lab/model-training.py Ultralytics Settings Reference The following settings.json configuration is used for Ultralytics: { \"settings_version\": \"0.0.6\", \"datasets_dir\": \"/home/xnorspx/Projects/Stream2Prompt/model-lab/ds\", \"weights_dir\": \"/home/xnorspx/Projects/Stream2Prompt/model-lab/weights\", \"runs_dir\": \"/home/xnorspx/Projects/Stream2Prompt/model-lab/runs\", \"uuid\": \"5bcc1cf32ada676916ec735cd4e216c62ff1e97244777357720c2ca28a7b7314\", \"sync\": true, \"api_key\": \"\", \"openai_api_key\": \"\", \"clearml\": true, \"comet\": true, \"dvc\": true, \"hub\": true, \"mlflow\": true, \"neptune\": true, \"raytune\": true, \"tensorboard\": false, \"wandb\": false, \"vscode_msg\": true, \"openvino_msg\": false } Model Evaluation To test and evaluate your trained models, run the evaluation script and select the model you want to test: uv run model-lab/model-eval.py The script will prompt you to choose from available trained models for evaluation.","title":"Usage"},{"location":"Model-Lab/Usage/#usage","text":"This guide provides instructions for using the components of the Model Lab, including data labeling, model training, and evaluation.","title":"Usage"},{"location":"Model-Lab/Usage/#data-labeling","text":"To prepare a new dataset for training, start Label Studio with the following command: uv run label-studio start This will launch the Label Studio web interface for data annotation and labeling.","title":"Data Labeling"},{"location":"Model-Lab/Usage/#model-training","text":"Configure the training parameters in model-lab/model-training.py Start the training process: uv run model-lab/model-training.py","title":"Model Training"},{"location":"Model-Lab/Usage/#ultralytics-settings-reference","text":"The following settings.json configuration is used for Ultralytics: { \"settings_version\": \"0.0.6\", \"datasets_dir\": \"/home/xnorspx/Projects/Stream2Prompt/model-lab/ds\", \"weights_dir\": \"/home/xnorspx/Projects/Stream2Prompt/model-lab/weights\", \"runs_dir\": \"/home/xnorspx/Projects/Stream2Prompt/model-lab/runs\", \"uuid\": \"5bcc1cf32ada676916ec735cd4e216c62ff1e97244777357720c2ca28a7b7314\", \"sync\": true, \"api_key\": \"\", \"openai_api_key\": \"\", \"clearml\": true, \"comet\": true, \"dvc\": true, \"hub\": true, \"mlflow\": true, \"neptune\": true, \"raytune\": true, \"tensorboard\": false, \"wandb\": false, \"vscode_msg\": true, \"openvino_msg\": false }","title":"Ultralytics Settings Reference"},{"location":"Model-Lab/Usage/#model-evaluation","text":"To test and evaluate your trained models, run the evaluation script and select the model you want to test: uv run model-lab/model-eval.py The script will prompt you to choose from available trained models for evaluation.","title":"Model Evaluation"}]}